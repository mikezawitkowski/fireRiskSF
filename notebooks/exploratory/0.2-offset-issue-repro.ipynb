{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The purpose of this notebook is to demonstrate clearly the problem with offset, to try and resolve it.\n",
    "\n",
    "By default, querying the public API of data.sfgov.org will only return the first 1,000 rows of data. That's fine and is good practice, but the problem is that it should be easy to paginate, and not get duplicate data.\n",
    "\n",
    "According to the API documentation, it's important to use `$order` when using `$offset` so that the data is returned in the same order. \n",
    "\n",
    "What's bizarre is that it seems that any attempt to order by a specific row and then return the next offset doesn't necessarily give you the next batch.\n",
    "\n",
    "The API documentation (https://dev.socrata.com/docs/paging.html) cautioned that `order` is needed for paging:\n",
    "\n",
    "> Heads Up! The order of the results of a query are not implicitly ordered, \n",
    "> so if you're paging, make sure you provide an `order` clause or at a \n",
    "> minimum **&#36;order=:id**. That will guarantee that the order of your results \n",
    "> will be stable as you page through the dataset.\n",
    "\n",
    "The problem though is that the results do not appear to be stable.\n",
    "\n",
    "\n",
    "## Steps to Repro\n",
    "\n",
    "As shown below, the steps to reproduce the issue are:\n",
    " \n",
    " * run the query using `order` and `offset=0`\n",
    " * check the min and max of the ordered column\n",
    " * change the offset and run the query again\n",
    " * check the min and max of the ordered column\n",
    " \n",
    "#### Expected/Wanted:\n",
    "\n",
    " * The min and max values of the first query should not overlap with the subsequent offset\n",
    " \n",
    "#### Got:\n",
    "\n",
    " * There appears to be quite a bit of overlap!\n",
    "\n",
    "\n",
    "### Update:\n",
    "\n",
    "I was in touch with someone at Socrates on the IRC channel, and here's what came up:\n",
    "\n",
    "```https://data.sfgov.org/resource/wbb6-uh78.json?$order=close_dttm%20DESC&$offset=0&$limit=3000```\n",
    "\n",
    "Thatâ€™d get all 3000\n",
    "\n",
    "If you want to get them in pages, change `$limit and $offset`\n",
    "\n",
    "...and the problem is still there... The offset still only retrieves 3,000 within the same close_dttm date range 5/27 through 7/10. \n",
    "\n",
    "### Update Number 2:\n",
    "\n",
    "I'm an idiot. The reason it is acting like this is because offset should be at 3,000, it is NOT the page number! \n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "Original Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function # always future proof for python3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here is what is meant to be the last 1,000 rows ordered by the close date time\n",
    "df = pd.read_json('https://data.sfgov.org/resource/wbb6-uh78.json?$order=close_dttm%20DESC&$offset=0&$limit=1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2016-06-24T16:28:44.000'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.close_dttm.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2016-07-10T22:08:15.000'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.close_dttm.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above dates, my next step was to do an offset and try again, and what I expected was that the next 1,000 rows of data would be from the dates that are all greater than the max or less than the min above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and here is the next 1,000 dates\n",
    "df = pd.read_json('https://data.sfgov.org/resource/wbb6-uh78.json?$order=close_dttm%20DESC&$offset=1&$limit=1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2016-06-24T16:05:07.000'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.close_dttm.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2016-07-10T19:52:41.000'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.close_dttm.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the results are slightly different than the first, but it's still within the same date range. This shows I'm doing something really wrong.\n",
    "\n",
    "# The Solution\n",
    "\n",
    "After some back and forth with @chrismetcalf on the IRC channel for Socrates API help, it was revealed that my error was really really basic. I was treating offset as a page number, when it (strangely enough) is just an offset. So if I want the next 1,000 items, I need the offset to be at 1,000. And then page 2 will be the offset of 2,000. \n",
    "\n",
    "It was so simple!\n",
    "\n",
    "OK, back to work, but first here is the solution correctly implemented:\n",
    "\n",
    "(notice how the df2 min and max all took place before the minimum of the first df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('https://data.sfgov.org/resource/wbb6-uh78.json?$order=close_dttm%20DESC&$offset=0&$limit=1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-24T16:28:44.000\n",
      "2016-07-10T22:08:15.000\n"
     ]
    }
   ],
   "source": [
    "print(df.close_dttm.min())\n",
    "print(df.close_dttm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_json('https://data.sfgov.org/resource/wbb6-uh78.json?$order=close_dttm%20DESC&$offset=1000&$limit=1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-10T11:37:39.000\n",
      "2016-06-24T16:05:07.000\n"
     ]
    }
   ],
   "source": [
    "print(df2.close_dttm.min())\n",
    "print(df2.close_dttm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
